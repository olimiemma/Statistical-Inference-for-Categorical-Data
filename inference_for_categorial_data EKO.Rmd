---
title: "Inference for categorical data"
author: "Emmanuel Kasigazi"
output:
  html_document:
    includes:
      in_header: header.html
    css: ./lab.css
    highlight: pygments
    theme: cerulean
    toc: true
    toc_float: true
  pdf_document: default
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(eval = TRUE, message = FALSE, warning = FALSE)
```

## Getting Started

### Load packages

In this lab, we will explore and visualize the data using the
**tidyverse** suite of packages, and perform statistical inference using
**infer**. The data can be found in the companion package for OpenIntro
resources, **openintro**.

Let's load the packages.

```{r load-packages}
library(tidyverse)
library(openintro)
library(infer)
library(tidyverse)
library(openintro)
library(infer)
library(infer)
library(dplyr)
library(tinytex)
```

### The data

You will be analyzing the same dataset as in the previous lab, where you
delved into a sample from the Youth Risk Behavior Surveillance System
(YRBSS) survey, which uses data from high schoolers to help discover
health patterns. The dataset is called `yrbss`.

1.  What are the counts within each category for the amount of days
    these students have texted while driving within the past 30 days?

```{r}
# Load the yrbss dataset
data(yrbss)

# Check if the dataset is loaded correctly
head(yrbss)

```

```{r}
yrbss %>%
  count(text_while_driving_30d)
```

**text_while_driving_30d n** <chr><int> **1 0 4792 2 1-2 925 3 10-19 373
4 20-29 298 5 3-5 493 6 30 827 7 6-9 311 8 did not drive 4646 9 NA 918**

What is the proportion of people who have texted while driving every day
in the past 30 days and never wear helmets? **\*\* 0.03408673
"Percentage: 3.41%" "Count: 463"**

```{r}
# Calculate the proportion
proportion <- sum(yrbss$text_while_driving_30d == "30" & yrbss$helmet_12m == "never", na.rm = TRUE) / nrow(yrbss)

# Display the proportion
print(proportion)

# To get the percentage
percentage <- proportion * 100
print(paste0("Percentage: ", round(percentage, 2), "%"))

# To see the actual count
count <- sum(yrbss$text_while_driving_30d == "30" & yrbss$helmet_12m == "never", na.rm = TRUE)
print(paste0("Count: ", count))
```

Remember that you can use `filter` to limit the dataset to just
non-helmet wearers. Here, we will name the dataset `no_helmet`.

```{r no helmet}
data('yrbss', package='openintro')
no_helmet <- yrbss %>%
  filter(helmet_12m == "never")
```

Also, it may be easier to calculate the proportion if you create a new
variable that specifies whether the individual has texted every day
while driving over the past 30 days or not. We will call this variable
`text_ind`.

```{r indicator-texting}
no_helmet <- no_helmet %>%
  mutate(text_ind = ifelse(text_while_driving_30d == "30", "yes", "no"))
```

## Inference on proportions

When summarizing the YRBSS, the Centers for Disease Control and
Prevention seeks insight into the population *parameters*. To do this,
you can answer the question, "What proportion of people in your sample
reported that they have texted while driving each day for the past 30
days?" with a statistic; while the question "What proportion of people
on earth have texted while driving each day for the past 30 days?" is
answered with an estimate of the parameter.

The inferential tools for estimating population proportion are analogous
to those used for means in the last chapter: the confidence interval and
the hypothesis test.

```{r nohelmet-text-ci}
no_helmet %>%
  drop_na(text_ind) %>% # Drop missing values
  specify(response = text_ind, success = "yes") %>%
  generate(reps = 1000, type = "bootstrap") %>%
  calculate(stat = "prop") %>%
  get_ci(level = 0.95)
```

Note that since the goal is to construct an interval estimate for a
proportion, it's necessary to both include the `success` argument within
`specify`, which accounts for the proportion of non-helmet wearers than
have consistently texted while driving the past 30 days, in this
example, and that `stat` within `calculate` is here "prop", signaling
that you are trying to do some sort of inference on a proportion.

3.  What is the margin of error for the estimate of the proportion of
    non-helmet wearers that have texted while driving each day for the
    past 30 days based on this survey?

```{r}
# Margin of error calculation from the confidence interval
upper_ci <- 0.0773
lower_ci <- 0.0654

# Margin of error is half the width of the confidence interval
margin_of_error <- (upper_ci - lower_ci) / 2

# Display the result
print(margin_of_error)
```

**[1] 0.00595**

4.  Using the `infer` package, calculate confidence intervals for two
    other categorical variables (you'll need to decide which level to
    call "success", and report the associated margins of error. Interpet
    the interval in context of the data. It may be helpful to create new
    data sets for each of the two countries first, and then use these
    data sets to construct the confidence intervals.

**\*\*For the first example, I'm calculating the confidence interval for
the proportion of students who are physically active all 7 days of the
week.**

**"success" is defined as being active every day The output will include
a 95% confidence interval and the margin of error**

**For the second example, I'm calculating the confidence interval for
the proportion of students who get 8 or more hours of sleep on school
nights.**

**"success" is defined as getting 8+ hours of sleep The output will
include a 95% confidence interval and the margin of error\*\***

```{r}
# Example 1: Confidence interval for proportion of students who are physically active 7 days/week
# Create indicator for those who are active every day (7 days)
physically_active <- yrbss %>%
  mutate(active_daily = ifelse(physically_active_7d == "7", "yes", "no"))
```

```{r}
# Calculate 95% confidence interval
active_ci <- physically_active %>%
  drop_na(active_daily) %>%
  specify(response = active_daily, success = "yes") %>%
  generate(reps = 1000, type = "bootstrap") %>%
  calculate(stat = "prop") %>%
  get_ci(level = 0.95)
```

```{r}
# Display the confidence interval
print(active_ci)
```

**lower_ci upper_ci** <dbl><dbl>

**0.264 0.280**

```{r}
# Calculate margin of error
active_margin <- (active_ci$upper_ci - active_ci$lower_ci) / 2
print(paste("Margin of error for physically active daily:", active_margin))
```

**"Margin of error for physically active daily: 0.00792824943651391"**

```{r}
# Example 2: Confidence interval for proportion of students who get 8+ hours of sleep on school nights
# Create indicator for those who get 8+ hours of sleep
sleep_data <- yrbss %>%
  mutate(enough_sleep = ifelse(school_night_hours_sleep >= "8", "yes", "no"))
```

```{r}
# Calculate 95% confidence interval
sleep_ci <- sleep_data %>%
  drop_na(enough_sleep) %>%
  specify(response = enough_sleep, success = "yes") %>%
  generate(reps = 1000, type = "bootstrap") %>%
  calculate(stat = "prop") %>%
  get_ci(level = 0.95)

# Display the confidence interval
print(sleep_ci)
```

<dbl><dbl>

**lower_ci 0.27**

**upper_ci 0.288**

```{r}
# Calculate margin of error
sleep_margin <- (sleep_ci$upper_ci - sleep_ci$lower_ci) / 2
print(paste("Margin of error for 8+ hours of sleep:", sleep_margin))
```

**"Margin of error for 8+ hours of sleep: 0.00823267126064045"**

## How does the proportion affect the margin of error?

Imagine you've set out to survey 1000 people on two questions: are you
at least 6-feet tall? and are you left-handed? Since both of these
sample proportions were calculated from the same sample size, they
should have the same margin of error, right? Wrong! While the margin of
error does change with sample size, it is also affected by the
proportion.

Think back to the formula for the standard error:
$SE = \sqrt{p(1-p)/n}$. This is then used in the formula for the margin
of error for a 95% confidence interval:

$$
ME = 1.96\times SE = 1.96\times\sqrt{p(1-p)/n} \,.
$$ Since the population proportion $p$ is in this $ME$ formula, it
should make sense that the margin of error is in some way dependent on
the population proportion. We can visualize this relationship by
creating a plot of $ME$ vs. $p$.

Since sample size is irrelevant to this discussion, let's just set it to
some value ($n = 1000$) and use this value in the following
calculations:

```{r n-for-me-plot}
n <- 1000
```

The first step is to make a variable `p` that is a sequence from 0 to 1
with each number incremented by 0.01. You can then create a variable of
the margin of error (`me`) associated with each of these values of `p`
using the familiar approximate formula ($ME = 2 \times SE$).

```{r p-me}
p <- seq(from = 0, to = 1, by = 0.01)
me <- 2 * sqrt(p * (1 - p)/n)
```

Lastly, you can plot the two variables against each other to reveal
their relationship. To do so, we need to first put these variables in a
data frame that you can call in the `ggplot` function.

```{r me-plot}
dd <- data.frame(p = p, me = me)
ggplot(data = dd, aes(x = p, y = me)) + 
  geom_line() +
  labs(x = "Population Proportion", y = "Margin of Error")
```

5.  Describe the relationship between `p` and `me`. Include the margin
    of error vs. population proportion plot you constructed in your
    answer. For a given sample size, for which value of `p` is margin of
    error maximized?

**The relationship between the population proportion (p) and the margin
of error (me) is parabolic. As shown in the plot, the margin of error is
at its maximum when p = 0.5 and decreases symmetrically as p approaches
either 0 or 1.**

**This relationship can be explained by the formula for margin of
error:**

**ME = 1.96 × √(p(1-p)/n)**

**When we look at the term p(1-p) within this formula: - When p = 0.5:
p(1-p) = 0.5 × 0.5 = 0.25 (maximum value) - When p approaches 0 or 1:
p(1-p) approaches 0**

**For a given sample size, the margin of error is maximized when p = 0.5
(50%). This makes intuitive sense because a proportion of 0.5 represents
maximum variability in a binary outcome - it's essentially a 50/50
split, which creates the most uncertainty in our estimate.**

**To demonstrate this mathematically, we can take the derivative of the
p(1-p) function with respect to p, set it equal to zero, and solve for
p: - d/dp[p(1-p)] = 1-2p = 0 - 1-2p = 0 - p = 0.5**

**This confirms that the margin of error reaches its maximum value when
the population proportion is 0.5.**

**In practical terms, this means that when designing a study where you
expect the proportion to be close to 0.5, you'll need a larger sample
size to achieve the same precision compared to a study where the
expected proportion is closer to 0 or 1.\*\***

## Success-failure condition

We have emphasized that you must always check conditions before making
inference. For inference on proportions, the sample proportion can be
assumed to be nearly normal if it is based upon a random sample of
independent observations and if both $np \geq 10$ and
$n(1 - p) \geq 10$. This rule of thumb is easy enough to follow, but it
makes you wonder: what's so special about the number 10?

The short answer is: nothing. You could argue that you would be fine
with 9 or that you really should be using 11. What is the "best" value
for such a rule of thumb is, at least to some degree, arbitrary.
However, when $np$ and $n(1-p)$ reaches 10 the sampling distribution is
sufficiently normal to use confidence intervals and hypothesis tests
that are based on that approximation.

You can investigate the interplay between $n$ and $p$ and the shape of
the sampling distribution by using simulations. Play around with the
following app to investigate how the shape, center, and spread of the
distribution of $\hat{p}$ changes as $n$ and $p$ changes.

```{r sf-app, echo=FALSE, eval=FALSE}
library(shiny)
shinyApp(
  ui = fluidPage(
      numericInput("n", label = "Sample size:", value = 300),
      
      sliderInput("p", label = "Population proportion:",
                  min = 0, max = 1, value = 0.1, step = 0.01),
      
      numericInput("x_min", label = "Min for x-axis:", value = 0, min = 0, max = 1),
      numericInput("x_max", label = "Max for x-axis:", value = 1, min = 0, max = 1),
    plotOutput('plotOutput')
  ),
  
  server = function(input, output) { 
    output$plotOutput = renderPlot({
      pp <- data.frame(p_hat = rep(0, 5000))
      for(i in 1:5000){
        samp <- sample(c(TRUE, FALSE), input$n, replace = TRUE, 
                       prob = c(input$p, 1 - input$p))
        pp$p_hat[i] <- sum(samp == TRUE) / input$n
      }
      bw <- diff(range(pp$p_hat)) / 30
      ggplot(data = pp, aes(x = p_hat)) +
        geom_histogram(binwidth = bw) +
        xlim(input$x_min, input$x_max) +
        ggtitle(paste0("Distribution of p_hats, drawn from p = ", input$p, ", n = ", input$n))
    })
  },
  
  options = list(height = 500)
)
```

6.  Describe the sampling distribution of sample proportions at
    $n = 300$ and $p = 0.1$. Be sure to note the center, spread, and
    shape.

\*\***The sampling distribution of sample proportions at n = 300 and p =
0.1 has the following characteristics:**

**Center: The sampling distribution is centered at p = 0.1, which is the
true population proportion. This is visible in the histogram, where the
peak is centered around 0.1 on the x-axis. Spread: The standard error of
the sampling distribution is √(p(1-p)/n) = √(0.1 × 0.9/300) ≈ 0.0173.
This relatively small spread is reflected in the histogram, where most
of the sample proportions fall within a narrow range around 0.1, roughly
between 0.05 and 0.15. Shape: The sampling distribution is approximately
normal, following the bell-shaped curve characteristic of a normal
distribution. This confirms that the Central Limit Theorem applies since
both np = 30 and n(1-p) = 270 greatly exceed the threshold of 10
required by the success-failure condition.**

**The histogram shows some natural sampling variability but clearly
demonstrates the properties expected from statistical theory - a normal
distribution centered at the true population proportion with a spread
determined by the standard error formula.\*\***

7.  Keep $n$ constant and change $p$. How does the shape, center, and
    spread of the sampling distribution vary as $p$ changes. You might
    want to adjust min and max for the $x$-axis for a better view of the
    distribution.

**Spread: The spread (standard error) follows the formula SE =
√(p(1-p)/n). This creates an interesting pattern:**

**The spread is smallest when p is near 0 or near 1 The spread increases
as p approaches 0.5 The spread reaches its maximum when p = 0.5 The
relationship between p and spread forms a parabolic shape, similar to
the margin of error relationship we saw earlier.**

**Shape: The sampling distribution is approximately normal as long as
the success-failure condition is met (np ≥ 10 and n(1-p) ≥ 10). When p
is very close to 0 or 1, the distribution becomes slightly skewed
(right-skewed for small p, left-skewed for large p) until p is extreme
enough that the condition is violated.**

**Center: The center of the sampling distribution always equals the true
population proportion p. As p increases from near 0 to 0.5 and then to
near 1, the center of the distribution shifts accordingly along the
x-axis.**

**This behavior reinforces what we learned about the margin of error:
proportions near 0.5 have the most variability in their estimates, while
proportions near 0 or 1 have less variability and therefore produce more
precise estimates.**

8.  Now also change $n$. How does $n$ appear to affect the distribution
    of $\hat{p}$? When we change both p and n, we observe several key
    effects on the sampling distribution of p̂:

    **Effect of n (sample size):**

    -   **As n increases, the spread of the sampling distribution
        decreases, making it narrower and more concentrated around the
        true value p**
    -   **The standard error formula SE = √(p(1-p)/n) shows this
        relationship explicitly - larger n leads to smaller standard
        error**
    -   **The distribution becomes more precisely centered on the true
        proportion**
    -   **With very large n, even distributions with extreme values of p
        become approximately normal**

    **Combined effects of changing both p and n:**

    -   **For any value of p, increasing n makes the distribution
        narrower**
    -   **For any value of n, the distribution is widest when p = 0.5
        and narrowest when p is close to 0 or 1**
    -   **Smaller sample sizes (low n) with extreme proportions (p close
        to 0 or 1) may violate the success-failure condition (np ≥ 10
        and n(1-p) ≥ 10)**

    **Practical implications:**

    -   **Larger sample sizes always provide more precise estimates
        (smaller SE) regardless of the true proportion**
    -   **When designing studies where p is expected to be near 0.5,
        larger sample sizes are needed to achieve the same precision as
        when p is near 0 or 1**
    -   **For rare events (p near 0 or 1), you need larger sample sizes
        to ensure the success-failure condition is met**

**In summary, n affects the spread of the distribution in an inverse
square root relationship (SE ∝ 1/√n), while the effect of p follows a
parabolic relationship with maximum spread at p = 0.5. Together, these
parameters determine both the shape and precision of the sampling
distribution.**

------------------------------------------------------------------------

## More Practice

For some of the exercises below, you will conduct inference comparing
two proportions. In such cases, you have a response variable that is
categorical, and an explanatory variable that is also categorical, and
you are comparing the proportions of success of the response variable
across the levels of the explanatory variable. This means that when
using `infer`, you need to include both variables within `specify`.

9.  Is there convincing evidence that those who sleep 10+ hours per day
    are more likely to strength train every day of the week? As always,
    write out the hypotheses for any tests you conduct and outline the
    status of the conditions for inference. If you find a significant
    difference, also quantify this difference with a confidence
    interval. First, let's write out the hypotheses: H₀: There is no
    difference in the proportion of people who strength train every day
    between those who sleep 10+ hours and those who sleep less than 10
    hours. H₁: The proportion of people who strength train every day is
    higher among those who sleep 10+ hours compared to those who sleep
    less than 10 hours.

Conditions for inference:

Random sampling: The YRBSS dataset uses a complex sampling design to be
representative of high school students Independence: Each observation is
from a different student Success-failure condition: Need to verify np ≥
10 and n(1-p) ≥ 10 for both groups

```{r}
# Create indicator variables
yrbss_modified <- yrbss %>%
  mutate(
    sleep_10plus = ifelse(school_night_hours_sleep >= "10", "yes", "no"),
    strength_daily = ifelse(strength_training_7d == "7", "yes", "no")
  )
```

```{r}
# Drop missing values
yrbss_complete <- yrbss_modified %>%
  drop_na(sleep_10plus, strength_daily)
```

```{r}
# Check the success-failure condition
sleep_10plus_counts <- yrbss_complete %>%
  group_by(sleep_10plus) %>%
  summarize(
    n = n(),
    strength_daily_count = sum(strength_daily == "yes"),
    proportion = mean(strength_daily == "yes")
  )
```

```{r}
# Calculate observed difference in proportions
obs_diff <- sleep_10plus_counts %>%
  summarize(diff = proportion[sleep_10plus == "yes"] - proportion[sleep_10plus == "no"]) %>%
  pull(diff)
```

```{r}
# Conduct hypothesis test using infer
p_value <- yrbss_complete %>%
  specify(strength_daily ~ sleep_10plus, success = "yes") %>%
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute") %>%
  calculate(stat = "diff in props", order = c("yes", "no")) %>%
  get_p_value(obs_stat = obs_diff, direction = "greater")
print(paste("P-value:", p_value))
```

```{r}
# Calculate 95% confidence interval
ci_result <- yrbss_complete %>%
  specify(strength_daily ~ sleep_10plus, success = "yes") %>%
  generate(reps = 1000, type = "bootstrap") %>%
  calculate(stat = "diff in props", order = c("yes", "no")) %>%
  get_ci(level = 0.95)
print("95% Confidence Interval:")
print(ci_result)
```

```{r}
# Print conclusion
alpha <- 0.05
if (p_value < alpha) {
  print(paste("Reject the null hypothesis (p-value =", p_value, "< alpha =", alpha, ")"))
  print("There is convincing evidence that those who sleep 10+ hours per day are more likely to strength train every day.")
} else {
  print(paste("Fail to reject the null hypothesis (p-value =", p_value, ">= alpha =", alpha, ")"))
  print("There is not convincing evidence that those who sleep 10+ hours per day are more likely to strength train every day.")
}
```

**we fail to reject the null hypothesis. There is not convincing
evidence that those who sleep 10+ hours per day are more likely to
strength train every day of the week**

10. Let's say there has been no difference in likeliness to strength
    train every day of the week for those who sleep 10+ hours. What is
    the probablity that you could detect a change (at a significance
    level of 0.05) simply by chance? *Hint:* Review the definition of
    the Type 1 error.

    **The probability of detecting a change (rejecting the null
    hypothesis) when there is actually no difference (the null
    hypothesis is true) is exactly the significance level, which is 0.05
    or 5% in this case.**

**This is the definition of a Type I error: incorrectly rejecting a true
null hypothesis. The significance level (alpha) that we set at 0.05 is
precisely the probability we're willing to accept for making this type
of error.**

**In other words, if there is truly no difference in the likelihood of
strength training every day between those who sleep 10+ hours and those
who don't, we would still expect to find a "statistically significant"
difference about 5% of the time simply due to random sampling
variation.**

**This is why we can never "prove" the null hypothesis - we can only
fail to reject it. Even when a study finds a "significant" result with p
\< 0.05, there's still a 5% chance that the finding is just due to
random chance when the null hypothesis is actually true.**

11. Suppose you're hired by the local government to estimate the
    proportion of residents that attend a religious service on a weekly
    basis. According to the guidelines, the estimate must have a margin
    of error no greater than 1% with 95% confidence. You have no idea
    what to expect for $p$. How many people would you have to sample to
    ensure that you are within the guidelines?\
    *Hint:* Refer to your plot of the relationship between $p$ and
    margin of error. This question does not require using a dataset.

**To determine the required sample size for estimating a proportion with
a margin of error no greater than 1% at 95% confidence, I need to work
with the margin of error formula:**

**ME = 1.96 × √(p(1-p)/n)**

**Where: - ME is the margin of error (0.01 or 1%) - 1.96 is the z-score
for 95% confidence - p is the population proportion - n is the required
sample size**

**Since I don't know what to expect for p, I need to use a conservative
approach. From the plot of the relationship between p and margin of
error, we know that the margin of error is maximized when p = 0.5.
Therefore, to ensure the margin of error doesn't exceed 1% regardless of
the true proportion, I'll use p = 0.5 in my calculation.**

**I can solve for n:**

**0.01 = 1.96 × √(0.5 × 0.5/n) 0.01 = 1.96 × √(0.25/n) 0.01 = 1.96 ×
0.5/√n 0.01 × √n = 1.96 × 0.5 0.01 × √n = 0.98 √n = 0.98/0.01 √n = 98 n
= 9,604**

**Therefore, I would need to sample at least 9,604 people to ensure a
margin of error no greater than 1% with 95% confidence, regardless of
what the true proportion turns out to be.**

------------------------------------------------------------------------
